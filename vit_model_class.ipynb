{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e84cf0fe-45ff-4786-afb1-4e1e12b0197b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.3.1)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.18.1)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Downloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tzdata, threadpoolctl, scipy, joblib, scikit-learn, pandas\n",
      "Successfully installed joblib-1.4.2 pandas-2.2.2 scikit-learn-1.5.1 scipy-1.14.1 threadpoolctl-3.5.0 tzdata-2024.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6cd73b-adc3-4660-939b-975d5bbee4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /workspace/hackathon-online-agriculture-classification\n",
      "Attempting to load CSV from: /workspace/hackathon-online-agriculture-classification/train.csv\n",
      "Successfully loaded CSV with 38511 entries\n",
      "Image directory: /workspace/hackathon-online-agriculture-classification/Images\n",
      "Dataset split - Train: 30808, Validation: 3851, Test: 3852\n",
      "Verifying data loading...\n",
      "Batch shape: torch.Size([32, 3, 224, 224])\n",
      "Labels: tensor([19, 38, 32, 26, 39, 18, 27, 37, 19, 19, 19, 41, 39, 33, 29,  2, 39, 19,\n",
      "         0, 14,  2, 32, 41, 39, 41, 29,  3, 39, 23, 26, 41, 22])\n",
      "Using device: cuda\n",
      "Starting training for 100 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - Training: 100%|██████████| 963/963 [02:21<00:00,  6.82it/s]\n",
      "Epoch 1/100 - Validation: 100%|██████████| 121/121 [00:06<00:00, 17.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Train Loss: 0.2050\n",
      "Val Loss: 0.0542\n",
      "Val F1-Score: 0.9810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100 - Training: 100%|██████████| 963/963 [02:22<00:00,  6.75it/s]\n",
      "Epoch 2/100 - Validation: 100%|██████████| 121/121 [00:06<00:00, 17.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "Train Loss: 0.0538\n",
      "Val Loss: 0.0799\n",
      "Val F1-Score: 0.9777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100 - Training: 100%|██████████| 963/963 [02:23<00:00,  6.72it/s]\n",
      "Epoch 3/100 - Validation: 100%|██████████| 121/121 [00:06<00:00, 18.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100\n",
      "Train Loss: 0.0476\n",
      "Val Loss: 0.0460\n",
      "Val F1-Score: 0.9846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100 - Training: 100%|██████████| 963/963 [02:22<00:00,  6.74it/s]\n",
      "Epoch 4/100 - Validation: 100%|██████████| 121/121 [00:06<00:00, 18.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100\n",
      "Train Loss: 0.0354\n",
      "Val Loss: 0.1045\n",
      "Val F1-Score: 0.9718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100 - Training: 100%|██████████| 963/963 [02:21<00:00,  6.82it/s]\n",
      "Epoch 5/100 - Validation: 100%|██████████| 121/121 [00:06<00:00, 17.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n",
      "Train Loss: 0.0362\n",
      "Val Loss: 0.1055\n",
      "Val F1-Score: 0.9665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/100 - Training: 100%|██████████| 963/963 [02:24<00:00,  6.69it/s]\n",
      "Epoch 6/100 - Validation: 100%|██████████| 121/121 [00:06<00:00, 17.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n",
      "Train Loss: 0.0291\n",
      "Val Loss: 0.1505\n",
      "Val F1-Score: 0.9654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/100 - Training: 100%|██████████| 963/963 [02:22<00:00,  6.77it/s]\n",
      "Epoch 7/100 - Validation: 100%|██████████| 121/121 [00:06<00:00, 17.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100\n",
      "Train Loss: 0.0290\n",
      "Val Loss: 0.0392\n",
      "Val F1-Score: 0.9875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/100 - Training: 100%|██████████| 963/963 [02:22<00:00,  6.74it/s]\n",
      "Epoch 8/100 - Validation: 100%|██████████| 121/121 [00:06<00:00, 17.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100\n",
      "Train Loss: 0.0246\n",
      "Val Loss: 0.0889\n",
      "Val F1-Score: 0.9732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/100 - Training: 100%|██████████| 963/963 [02:22<00:00,  6.74it/s]\n",
      "Epoch 9/100 - Validation: 100%|██████████| 121/121 [00:06<00:00, 17.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100\n",
      "Train Loss: 0.0164\n",
      "Val Loss: 0.0515\n",
      "Val F1-Score: 0.9811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/100 - Training: 100%|██████████| 963/963 [02:21<00:00,  6.82it/s]\n",
      "Epoch 10/100 - Validation: 100%|██████████| 121/121 [00:06<00:00, 18.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100\n",
      "Train Loss: 0.0323\n",
      "Val Loss: 0.0714\n",
      "Val F1-Score: 0.9778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/100 - Training: 100%|██████████| 963/963 [02:22<00:00,  6.75it/s]\n",
      "Epoch 11/100 - Validation: 100%|██████████| 121/121 [00:06<00:00, 17.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100\n",
      "Train Loss: 0.0222\n",
      "Val Loss: 0.0456\n",
      "Val F1-Score: 0.9848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/100 - Training: 100%|██████████| 963/963 [02:22<00:00,  6.74it/s]\n",
      "Epoch 12/100 - Validation: 100%|██████████| 121/121 [00:06<00:00, 17.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100\n",
      "Train Loss: 0.0026\n",
      "Val Loss: 0.0153\n",
      "Val F1-Score: 0.9951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/100 - Training: 100%|██████████| 963/963 [02:23<00:00,  6.72it/s]\n",
      "Epoch 13/100 - Validation: 100%|██████████| 121/121 [00:06<00:00, 17.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100\n",
      "Train Loss: 0.0006\n",
      "Val Loss: 0.0129\n",
      "Val F1-Score: 0.9961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/100 - Training: 100%|██████████| 963/963 [02:21<00:00,  6.82it/s]\n",
      "Epoch 14/100 - Validation: 100%|██████████| 121/121 [00:06<00:00, 17.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100\n",
      "Train Loss: 0.0004\n",
      "Val Loss: 0.0129\n",
      "Val F1-Score: 0.9956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/100 - Training: 100%|██████████| 963/963 [02:23<00:00,  6.73it/s]\n",
      "Epoch 15/100 - Validation: 100%|██████████| 121/121 [00:06<00:00, 17.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100\n",
      "Train Loss: 0.0003\n",
      "Val Loss: 0.0125\n",
      "Val F1-Score: 0.9961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/100 - Training: 100%|██████████| 963/963 [02:22<00:00,  6.74it/s]\n",
      "Epoch 16/100 - Validation: 100%|██████████| 121/121 [00:06<00:00, 18.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100\n",
      "Train Loss: 0.0002\n",
      "Val Loss: 0.0120\n",
      "Val F1-Score: 0.9961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/100 - Training: 100%|██████████| 963/963 [02:21<00:00,  6.83it/s]\n",
      "Epoch 17/100 - Validation: 100%|██████████| 121/121 [00:06<00:00, 17.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100\n",
      "Train Loss: 0.0001\n",
      "Val Loss: 0.0127\n",
      "Val F1-Score: 0.9964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/100 - Training: 100%|██████████| 963/963 [02:23<00:00,  6.72it/s]\n",
      "Epoch 18/100 - Validation: 100%|██████████| 121/121 [00:06<00:00, 17.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100\n",
      "Train Loss: 0.0001\n",
      "Val Loss: 0.0126\n",
      "Val F1-Score: 0.9961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/100 - Training: 100%|██████████| 963/963 [02:23<00:00,  6.73it/s]\n",
      "Epoch 19/100 - Validation: 100%|██████████| 121/121 [00:06<00:00, 17.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100\n",
      "Train Loss: 0.0001\n",
      "Val Loss: 0.0132\n",
      "Val F1-Score: 0.9958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/100 - Training: 100%|██████████| 963/963 [02:22<00:00,  6.75it/s]\n",
      "Epoch 20/100 - Validation: 100%|██████████| 121/121 [00:06<00:00, 17.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100\n",
      "Train Loss: 0.0001\n",
      "Val Loss: 0.0130\n",
      "Val F1-Score: 0.9961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/100 - Training: 100%|██████████| 963/963 [02:23<00:00,  6.70it/s]\n",
      "Epoch 21/100 - Validation: 100%|██████████| 121/121 [00:06<00:00, 17.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100\n",
      "Train Loss: 0.0000\n",
      "Val Loss: 0.0132\n",
      "Val F1-Score: 0.9961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/100 - Training: 100%|██████████| 963/963 [02:23<00:00,  6.70it/s]\n",
      "Epoch 22/100 - Validation: 100%|██████████| 121/121 [00:06<00:00, 17.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100\n",
      "Train Loss: 0.0000\n",
      "Val Loss: 0.0133\n",
      "Val F1-Score: 0.9961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/100 - Training: 100%|██████████| 963/963 [02:23<00:00,  6.73it/s]\n",
      "Epoch 23/100 - Validation: 100%|██████████| 121/121 [00:06<00:00, 17.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/100\n",
      "Train Loss: 0.0000\n",
      "Val Loss: 0.0134\n",
      "Val F1-Score: 0.9961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/100 - Training: 100%|██████████| 963/963 [02:23<00:00,  6.71it/s]\n",
      "Epoch 24/100 - Validation: 100%|██████████| 121/121 [00:06<00:00, 17.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100\n",
      "Train Loss: 0.0000\n",
      "Val Loss: 0.0135\n",
      "Val F1-Score: 0.9961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/100 - Training:  91%|█████████▏| 879/963 [02:11<00:12,  6.82it/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "# 1. Load the CSV file\n",
    "csv_path = 'train.csv'\n",
    "print(f\"Attempting to load CSV from: {os.path.abspath(csv_path)}\")\n",
    "df = pd.read_csv(csv_path)\n",
    "print(f\"Successfully loaded CSV with {len(df)} entries\")\n",
    "\n",
    "# 2. Custom Dataset Class\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, df, image_dir, transform=None):\n",
    "        self.df = df\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            img_name = os.path.join(self.image_dir, self.df.iloc[idx, 0] + '.jpg')\n",
    "            image = Image.open(img_name).convert('RGB')\n",
    "            label = self.df.iloc[idx, 1]\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, label\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: File not found: {img_name}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_name}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "# 3. Enhanced Data Transformations with Augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 4. Create Dataset and DataLoader\n",
    "image_dir = 'Images'\n",
    "print(f\"Image directory: {os.path.abspath(image_dir)}\")\n",
    "full_dataset = ImageDataset(df, image_dir, transform=None)  # We'll apply transforms later\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = int(0.1 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Apply transforms\n",
    "train_dataset.dataset.transform = train_transform\n",
    "val_dataset.dataset.transform = val_test_transform\n",
    "test_dataset.dataset.transform = val_test_transform\n",
    "\n",
    "print(f\"Dataset split - Train: {train_size}, Validation: {val_size}, Test: {test_size}\")\n",
    "\n",
    "# 5. Model (Vision Transformer)\n",
    "def create_model(num_classes):\n",
    "    model = vit_b_16(weights=ViT_B_16_Weights.IMAGENET1K_V1)\n",
    "    \n",
    "    # Replace the last layer of the classifier\n",
    "    num_ftrs = model.heads[-1].in_features\n",
    "    model.heads[-1] = nn.Linear(num_ftrs, num_classes)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 6. Training function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device):\n",
    "    best_val_f1 = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_f1 = f1_score(val_labels, val_preds, average='weighted')\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Train Loss: {train_loss/len(train_loader):.4f}\")\n",
    "        print(f\"Val Loss: {val_loss/len(val_loader):.4f}\")\n",
    "        print(f\"Val F1-Score: {val_f1:.4f}\")\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            torch.save(model.state_dict(), 'best_model_vit.pth')\n",
    "\n",
    "    return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "    # Verify data loading\n",
    "    print(\"Verifying data loading...\")\n",
    "    for images, labels in train_loader:\n",
    "        print(\"Batch shape:\", images.shape)\n",
    "        print(\"Labels:\", labels)\n",
    "        break  # Just check the first batch\n",
    "\n",
    "    # Device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Create model\n",
    "    num_classes = 42  # Assuming 42 classes as in the original code\n",
    "    model = create_model(num_classes)\n",
    "    model.to(device)\n",
    "\n",
    "    # Loss and Optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.05)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.1)\n",
    "\n",
    "    # Train model\n",
    "    num_epochs = 100\n",
    "    print(f\"Starting training for {num_epochs} epochs...\")\n",
    "    model = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device)\n",
    "\n",
    "    # 8. Evaluation\n",
    "    print(\"Loading best model for evaluation...\")\n",
    "    model.load_state_dict(torch.load('best_model_vit.pth'))\n",
    "    model.eval()\n",
    "    test_preds = []\n",
    "    test_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            test_preds.extend(preds.cpu().numpy())\n",
    "            test_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # 9. Calculate F1-Score\n",
    "    test_f1 = f1_score(test_labels, test_preds, average='weighted')\n",
    "    print('Test F1-Score:', test_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a01393f2-c07b-44fd-982c-24352ac951f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for SwinTransformer:\n\tMissing key(s) in state_dict: \"features.0.0.weight\", \"features.0.0.bias\", \"features.0.2.weight\", \"features.0.2.bias\", \"features.1.0.norm1.weight\", \"features.1.0.norm1.bias\", \"features.1.0.attn.relative_position_bias_table\", \"features.1.0.attn.relative_position_index\", \"features.1.0.attn.qkv.weight\", \"features.1.0.attn.qkv.bias\", \"features.1.0.attn.proj.weight\", \"features.1.0.attn.proj.bias\", \"features.1.0.norm2.weight\", \"features.1.0.norm2.bias\", \"features.1.0.mlp.0.weight\", \"features.1.0.mlp.0.bias\", \"features.1.0.mlp.3.weight\", \"features.1.0.mlp.3.bias\", \"features.1.1.norm1.weight\", \"features.1.1.norm1.bias\", \"features.1.1.attn.relative_position_bias_table\", \"features.1.1.attn.relative_position_index\", \"features.1.1.attn.qkv.weight\", \"features.1.1.attn.qkv.bias\", \"features.1.1.attn.proj.weight\", \"features.1.1.attn.proj.bias\", \"features.1.1.norm2.weight\", \"features.1.1.norm2.bias\", \"features.1.1.mlp.0.weight\", \"features.1.1.mlp.0.bias\", \"features.1.1.mlp.3.weight\", \"features.1.1.mlp.3.bias\", \"features.2.reduction.weight\", \"features.2.norm.weight\", \"features.2.norm.bias\", \"features.3.0.norm1.weight\", \"features.3.0.norm1.bias\", \"features.3.0.attn.relative_position_bias_table\", \"features.3.0.attn.relative_position_index\", \"features.3.0.attn.qkv.weight\", \"features.3.0.attn.qkv.bias\", \"features.3.0.attn.proj.weight\", \"features.3.0.attn.proj.bias\", \"features.3.0.norm2.weight\", \"features.3.0.norm2.bias\", \"features.3.0.mlp.0.weight\", \"features.3.0.mlp.0.bias\", \"features.3.0.mlp.3.weight\", \"features.3.0.mlp.3.bias\", \"features.3.1.norm1.weight\", \"features.3.1.norm1.bias\", \"features.3.1.attn.relative_position_bias_table\", \"features.3.1.attn.relative_position_index\", \"features.3.1.attn.qkv.weight\", \"features.3.1.attn.qkv.bias\", \"features.3.1.attn.proj.weight\", \"features.3.1.attn.proj.bias\", \"features.3.1.norm2.weight\", \"features.3.1.norm2.bias\", \"features.3.1.mlp.0.weight\", \"features.3.1.mlp.0.bias\", \"features.3.1.mlp.3.weight\", \"features.3.1.mlp.3.bias\", \"features.4.reduction.weight\", \"features.4.norm.weight\", \"features.4.norm.bias\", \"features.5.0.norm1.weight\", \"features.5.0.norm1.bias\", \"features.5.0.attn.relative_position_bias_table\", \"features.5.0.attn.relative_position_index\", \"features.5.0.attn.qkv.weight\", \"features.5.0.attn.qkv.bias\", \"features.5.0.attn.proj.weight\", \"features.5.0.attn.proj.bias\", \"features.5.0.norm2.weight\", \"features.5.0.norm2.bias\", \"features.5.0.mlp.0.weight\", \"features.5.0.mlp.0.bias\", \"features.5.0.mlp.3.weight\", \"features.5.0.mlp.3.bias\", \"features.5.1.norm1.weight\", \"features.5.1.norm1.bias\", \"features.5.1.attn.relative_position_bias_table\", \"features.5.1.attn.relative_position_index\", \"features.5.1.attn.qkv.weight\", \"features.5.1.attn.qkv.bias\", \"features.5.1.attn.proj.weight\", \"features.5.1.attn.proj.bias\", \"features.5.1.norm2.weight\", \"features.5.1.norm2.bias\", \"features.5.1.mlp.0.weight\", \"features.5.1.mlp.0.bias\", \"features.5.1.mlp.3.weight\", \"features.5.1.mlp.3.bias\", \"features.5.2.norm1.weight\", \"features.5.2.norm1.bias\", \"features.5.2.attn.relative_position_bias_table\", \"features.5.2.attn.relative_position_index\", \"features.5.2.attn.qkv.weight\", \"features.5.2.attn.qkv.bias\", \"features.5.2.attn.proj.weight\", \"features.5.2.attn.proj.bias\", \"features.5.2.norm2.weight\", \"features.5.2.norm2.bias\", \"features.5.2.mlp.0.weight\", \"features.5.2.mlp.0.bias\", \"features.5.2.mlp.3.weight\", \"features.5.2.mlp.3.bias\", \"features.5.3.norm1.weight\", \"features.5.3.norm1.bias\", \"features.5.3.attn.relative_position_bias_table\", \"features.5.3.attn.relative_position_index\", \"features.5.3.attn.qkv.weight\", \"features.5.3.attn.qkv.bias\", \"features.5.3.attn.proj.weight\", \"features.5.3.attn.proj.bias\", \"features.5.3.norm2.weight\", \"features.5.3.norm2.bias\", \"features.5.3.mlp.0.weight\", \"features.5.3.mlp.0.bias\", \"features.5.3.mlp.3.weight\", \"features.5.3.mlp.3.bias\", \"features.5.4.norm1.weight\", \"features.5.4.norm1.bias\", \"features.5.4.attn.relative_position_bias_table\", \"features.5.4.attn.relative_position_index\", \"features.5.4.attn.qkv.weight\", \"features.5.4.attn.qkv.bias\", \"features.5.4.attn.proj.weight\", \"features.5.4.attn.proj.bias\", \"features.5.4.norm2.weight\", \"features.5.4.norm2.bias\", \"features.5.4.mlp.0.weight\", \"features.5.4.mlp.0.bias\", \"features.5.4.mlp.3.weight\", \"features.5.4.mlp.3.bias\", \"features.5.5.norm1.weight\", \"features.5.5.norm1.bias\", \"features.5.5.attn.relative_position_bias_table\", \"features.5.5.attn.relative_position_index\", \"features.5.5.attn.qkv.weight\", \"features.5.5.attn.qkv.bias\", \"features.5.5.attn.proj.weight\", \"features.5.5.attn.proj.bias\", \"features.5.5.norm2.weight\", \"features.5.5.norm2.bias\", \"features.5.5.mlp.0.weight\", \"features.5.5.mlp.0.bias\", \"features.5.5.mlp.3.weight\", \"features.5.5.mlp.3.bias\", \"features.6.reduction.weight\", \"features.6.norm.weight\", \"features.6.norm.bias\", \"features.7.0.norm1.weight\", \"features.7.0.norm1.bias\", \"features.7.0.attn.relative_position_bias_table\", \"features.7.0.attn.relative_position_index\", \"features.7.0.attn.qkv.weight\", \"features.7.0.attn.qkv.bias\", \"features.7.0.attn.proj.weight\", \"features.7.0.attn.proj.bias\", \"features.7.0.norm2.weight\", \"features.7.0.norm2.bias\", \"features.7.0.mlp.0.weight\", \"features.7.0.mlp.0.bias\", \"features.7.0.mlp.3.weight\", \"features.7.0.mlp.3.bias\", \"features.7.1.norm1.weight\", \"features.7.1.norm1.bias\", \"features.7.1.attn.relative_position_bias_table\", \"features.7.1.attn.relative_position_index\", \"features.7.1.attn.qkv.weight\", \"features.7.1.attn.qkv.bias\", \"features.7.1.attn.proj.weight\", \"features.7.1.attn.proj.bias\", \"features.7.1.norm2.weight\", \"features.7.1.norm2.bias\", \"features.7.1.mlp.0.weight\", \"features.7.1.mlp.0.bias\", \"features.7.1.mlp.3.weight\", \"features.7.1.mlp.3.bias\", \"norm.weight\", \"norm.bias\", \"head.weight\", \"head.bias\". \n\tUnexpected key(s) in state_dict: \"class_token\", \"conv_proj.weight\", \"conv_proj.bias\", \"encoder.pos_embedding\", \"encoder.layers.encoder_layer_0.ln_1.weight\", \"encoder.layers.encoder_layer_0.ln_1.bias\", \"encoder.layers.encoder_layer_0.self_attention.in_proj_weight\", \"encoder.layers.encoder_layer_0.self_attention.in_proj_bias\", \"encoder.layers.encoder_layer_0.self_attention.out_proj.weight\", \"encoder.layers.encoder_layer_0.self_attention.out_proj.bias\", \"encoder.layers.encoder_layer_0.ln_2.weight\", \"encoder.layers.encoder_layer_0.ln_2.bias\", \"encoder.layers.encoder_layer_0.mlp.0.weight\", \"encoder.layers.encoder_layer_0.mlp.0.bias\", \"encoder.layers.encoder_layer_0.mlp.3.weight\", \"encoder.layers.encoder_layer_0.mlp.3.bias\", \"encoder.layers.encoder_layer_1.ln_1.weight\", \"encoder.layers.encoder_layer_1.ln_1.bias\", \"encoder.layers.encoder_layer_1.self_attention.in_proj_weight\", \"encoder.layers.encoder_layer_1.self_attention.in_proj_bias\", \"encoder.layers.encoder_layer_1.self_attention.out_proj.weight\", \"encoder.layers.encoder_layer_1.self_attention.out_proj.bias\", \"encoder.layers.encoder_layer_1.ln_2.weight\", \"encoder.layers.encoder_layer_1.ln_2.bias\", \"encoder.layers.encoder_layer_1.mlp.0.weight\", \"encoder.layers.encoder_layer_1.mlp.0.bias\", \"encoder.layers.encoder_layer_1.mlp.3.weight\", \"encoder.layers.encoder_layer_1.mlp.3.bias\", \"encoder.layers.encoder_layer_2.ln_1.weight\", \"encoder.layers.encoder_layer_2.ln_1.bias\", \"encoder.layers.encoder_layer_2.self_attention.in_proj_weight\", \"encoder.layers.encoder_layer_2.self_attention.in_proj_bias\", \"encoder.layers.encoder_layer_2.self_attention.out_proj.weight\", \"encoder.layers.encoder_layer_2.self_attention.out_proj.bias\", \"encoder.layers.encoder_layer_2.ln_2.weight\", \"encoder.layers.encoder_layer_2.ln_2.bias\", \"encoder.layers.encoder_layer_2.mlp.0.weight\", \"encoder.layers.encoder_layer_2.mlp.0.bias\", \"encoder.layers.encoder_layer_2.mlp.3.weight\", \"encoder.layers.encoder_layer_2.mlp.3.bias\", \"encoder.layers.encoder_layer_3.ln_1.weight\", \"encoder.layers.encoder_layer_3.ln_1.bias\", \"encoder.layers.encoder_layer_3.self_attention.in_proj_weight\", \"encoder.layers.encoder_layer_3.self_attention.in_proj_bias\", \"encoder.layers.encoder_layer_3.self_attention.out_proj.weight\", \"encoder.layers.encoder_layer_3.self_attention.out_proj.bias\", \"encoder.layers.encoder_layer_3.ln_2.weight\", \"encoder.layers.encoder_layer_3.ln_2.bias\", \"encoder.layers.encoder_layer_3.mlp.0.weight\", \"encoder.layers.encoder_layer_3.mlp.0.bias\", \"encoder.layers.encoder_layer_3.mlp.3.weight\", \"encoder.layers.encoder_layer_3.mlp.3.bias\", \"encoder.layers.encoder_layer_4.ln_1.weight\", \"encoder.layers.encoder_layer_4.ln_1.bias\", \"encoder.layers.encoder_layer_4.self_attention.in_proj_weight\", \"encoder.layers.encoder_layer_4.self_attention.in_proj_bias\", \"encoder.layers.encoder_layer_4.self_attention.out_proj.weight\", \"encoder.layers.encoder_layer_4.self_attention.out_proj.bias\", \"encoder.layers.encoder_layer_4.ln_2.weight\", \"encoder.layers.encoder_layer_4.ln_2.bias\", \"encoder.layers.encoder_layer_4.mlp.0.weight\", \"encoder.layers.encoder_layer_4.mlp.0.bias\", \"encoder.layers.encoder_layer_4.mlp.3.weight\", \"encoder.layers.encoder_layer_4.mlp.3.bias\", \"encoder.layers.encoder_layer_5.ln_1.weight\", \"encoder.layers.encoder_layer_5.ln_1.bias\", \"encoder.layers.encoder_layer_5.self_attention.in_proj_weight\", \"encoder.layers.encoder_layer_5.self_attention.in_proj_bias\", \"encoder.layers.encoder_layer_5.self_attention.out_proj.weight\", \"encoder.layers.encoder_layer_5.self_attention.out_proj.bias\", \"encoder.layers.encoder_layer_5.ln_2.weight\", \"encoder.layers.encoder_layer_5.ln_2.bias\", \"encoder.layers.encoder_layer_5.mlp.0.weight\", \"encoder.layers.encoder_layer_5.mlp.0.bias\", \"encoder.layers.encoder_layer_5.mlp.3.weight\", \"encoder.layers.encoder_layer_5.mlp.3.bias\", \"encoder.layers.encoder_layer_6.ln_1.weight\", \"encoder.layers.encoder_layer_6.ln_1.bias\", \"encoder.layers.encoder_layer_6.self_attention.in_proj_weight\", \"encoder.layers.encoder_layer_6.self_attention.in_proj_bias\", \"encoder.layers.encoder_layer_6.self_attention.out_proj.weight\", \"encoder.layers.encoder_layer_6.self_attention.out_proj.bias\", \"encoder.layers.encoder_layer_6.ln_2.weight\", \"encoder.layers.encoder_layer_6.ln_2.bias\", \"encoder.layers.encoder_layer_6.mlp.0.weight\", \"encoder.layers.encoder_layer_6.mlp.0.bias\", \"encoder.layers.encoder_layer_6.mlp.3.weight\", \"encoder.layers.encoder_layer_6.mlp.3.bias\", \"encoder.layers.encoder_layer_7.ln_1.weight\", \"encoder.layers.encoder_layer_7.ln_1.bias\", \"encoder.layers.encoder_layer_7.self_attention.in_proj_weight\", \"encoder.layers.encoder_layer_7.self_attention.in_proj_bias\", \"encoder.layers.encoder_layer_7.self_attention.out_proj.weight\", \"encoder.layers.encoder_layer_7.self_attention.out_proj.bias\", \"encoder.layers.encoder_layer_7.ln_2.weight\", \"encoder.layers.encoder_layer_7.ln_2.bias\", \"encoder.layers.encoder_layer_7.mlp.0.weight\", \"encoder.layers.encoder_layer_7.mlp.0.bias\", \"encoder.layers.encoder_layer_7.mlp.3.weight\", \"encoder.layers.encoder_layer_7.mlp.3.bias\", \"encoder.layers.encoder_layer_8.ln_1.weight\", \"encoder.layers.encoder_layer_8.ln_1.bias\", \"encoder.layers.encoder_layer_8.self_attention.in_proj_weight\", \"encoder.layers.encoder_layer_8.self_attention.in_proj_bias\", \"encoder.layers.encoder_layer_8.self_attention.out_proj.weight\", \"encoder.layers.encoder_layer_8.self_attention.out_proj.bias\", \"encoder.layers.encoder_layer_8.ln_2.weight\", \"encoder.layers.encoder_layer_8.ln_2.bias\", \"encoder.layers.encoder_layer_8.mlp.0.weight\", \"encoder.layers.encoder_layer_8.mlp.0.bias\", \"encoder.layers.encoder_layer_8.mlp.3.weight\", \"encoder.layers.encoder_layer_8.mlp.3.bias\", \"encoder.layers.encoder_layer_9.ln_1.weight\", \"encoder.layers.encoder_layer_9.ln_1.bias\", \"encoder.layers.encoder_layer_9.self_attention.in_proj_weight\", \"encoder.layers.encoder_layer_9.self_attention.in_proj_bias\", \"encoder.layers.encoder_layer_9.self_attention.out_proj.weight\", \"encoder.layers.encoder_layer_9.self_attention.out_proj.bias\", \"encoder.layers.encoder_layer_9.ln_2.weight\", \"encoder.layers.encoder_layer_9.ln_2.bias\", \"encoder.layers.encoder_layer_9.mlp.0.weight\", \"encoder.layers.encoder_layer_9.mlp.0.bias\", \"encoder.layers.encoder_layer_9.mlp.3.weight\", \"encoder.layers.encoder_layer_9.mlp.3.bias\", \"encoder.layers.encoder_layer_10.ln_1.weight\", \"encoder.layers.encoder_layer_10.ln_1.bias\", \"encoder.layers.encoder_layer_10.self_attention.in_proj_weight\", \"encoder.layers.encoder_layer_10.self_attention.in_proj_bias\", \"encoder.layers.encoder_layer_10.self_attention.out_proj.weight\", \"encoder.layers.encoder_layer_10.self_attention.out_proj.bias\", \"encoder.layers.encoder_layer_10.ln_2.weight\", \"encoder.layers.encoder_layer_10.ln_2.bias\", \"encoder.layers.encoder_layer_10.mlp.0.weight\", \"encoder.layers.encoder_layer_10.mlp.0.bias\", \"encoder.layers.encoder_layer_10.mlp.3.weight\", \"encoder.layers.encoder_layer_10.mlp.3.bias\", \"encoder.layers.encoder_layer_11.ln_1.weight\", \"encoder.layers.encoder_layer_11.ln_1.bias\", \"encoder.layers.encoder_layer_11.self_attention.in_proj_weight\", \"encoder.layers.encoder_layer_11.self_attention.in_proj_bias\", \"encoder.layers.encoder_layer_11.self_attention.out_proj.weight\", \"encoder.layers.encoder_layer_11.self_attention.out_proj.bias\", \"encoder.layers.encoder_layer_11.ln_2.weight\", \"encoder.layers.encoder_layer_11.ln_2.bias\", \"encoder.layers.encoder_layer_11.mlp.0.weight\", \"encoder.layers.encoder_layer_11.mlp.0.bias\", \"encoder.layers.encoder_layer_11.mlp.3.weight\", \"encoder.layers.encoder_layer_11.mlp.3.bias\", \"encoder.ln.weight\", \"encoder.ln.bias\", \"heads.head.weight\", \"heads.head.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Load the state dictionary\u001b[39;00m\n\u001b[1;32m     30\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model_vit_h14.pth\u001b[39m\u001b[38;5;124m'\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 31\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     33\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:2189\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2184\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2185\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2186\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2190\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for SwinTransformer:\n\tMissing key(s) in state_dict: \"features.0.0.weight\", \"features.0.0.bias\", \"features.0.2.weight\", \"features.0.2.bias\", \"features.1.0.norm1.weight\", \"features.1.0.norm1.bias\", \"features.1.0.attn.relative_position_bias_table\", \"features.1.0.attn.relative_position_index\", \"features.1.0.attn.qkv.weight\", \"features.1.0.attn.qkv.bias\", \"features.1.0.attn.proj.weight\", \"features.1.0.attn.proj.bias\", \"features.1.0.norm2.weight\", \"features.1.0.norm2.bias\", \"features.1.0.mlp.0.weight\", \"features.1.0.mlp.0.bias\", \"features.1.0.mlp.3.weight\", \"features.1.0.mlp.3.bias\", \"features.1.1.norm1.weight\", \"features.1.1.norm1.bias\", \"features.1.1.attn.relative_position_bias_table\", \"features.1.1.attn.relative_position_index\", \"features.1.1.attn.qkv.weight\", \"features.1.1.attn.qkv.bias\", \"features.1.1.attn.proj.weight\", \"features.1.1.attn.proj.bias\", \"features.1.1.norm2.weight\", \"features.1.1.norm2.bias\", \"features.1.1.mlp.0.weight\", \"features.1.1.mlp.0.bias\", \"features.1.1.mlp.3.weight\", \"features.1.1.mlp.3.bias\", \"features.2.reduction.weight\", \"features.2.norm.weight\", \"features.2.norm.bias\", \"features.3.0.norm1.weight\", \"features.3.0.norm1.bias\", \"features.3.0.attn.relative_position_bias_table\", \"features.3.0.attn.relative_position_index\", \"features.3.0.attn.qkv.weight\", \"features.3.0.attn.qkv.bias\", \"features.3.0.attn.proj.weight\", \"features.3.0.attn.proj.bias\", \"features.3.0.norm2.weight\", \"features.3.0.norm2.bias\", \"features.3.0.mlp.0.weight\", \"features.3.0.mlp.0.bias\", \"features.3.0.mlp.3.weight\", \"features.3.0.mlp.3.bias\", \"features.3.1.norm1.weight\", \"features.3.1.norm1.bias\", \"features.3.1.attn.relative_position_bias_table\", \"features.3.1.attn.relative_position_index\", \"features.3.1.attn.qkv.weight\", \"features.3.1.attn.qkv.bias\", \"features.3.1.attn.proj.weight\", \"features.3.1.attn.proj.bias\", \"features.3.1.norm2.weight\", \"features.3.1.norm2.bias\", \"features.3.1.mlp.0.weight\", \"features.3.1.mlp.0.bias\", \"features.3.1.mlp.3.weight\", \"features.3.1.mlp.3.bias\", \"features.4.reduction.weight\", \"features.4.norm.weight\", \"features.4.norm.bias\", \"features.5.0.norm1.weight\", \"features.5.0.norm1.bias\", \"features.5.0.attn.relative_position_bias_table\", \"features.5.0.attn.relative_position_index\", \"features.5.0.attn.qkv.weight\", \"features.5.0.attn.qkv.bias\", \"features.5.0.attn.proj.weight\", \"features.5.0.attn.proj.bias\", \"features.5.0.norm2.weight\", \"features.5.0.norm2.bias\", \"features.5.0.mlp.0.weight\", \"features.5.0.mlp.0.bias\", \"features.5.0.mlp.3.weight\", \"features.5.0.mlp.3.bias\", \"features.5.1.norm1.weight\", \"features.5.1.norm1.bias\", \"features.5.1.attn.relative_position_bias_table\", \"features.5.1.attn.relative_position_index\", \"features.5.1.attn.qkv.weight\", \"features.5.1.attn.qkv.bias\", \"features.5.1.attn.proj.weight\", \"features.5.1.attn.proj.bias\", \"features.5.1.norm2.weight\", \"features.5.1.norm2.bias\", \"features.5.1.mlp.0.weight\", \"features.5.1.mlp.0.bias\", \"features.5.1.mlp.3.weight\", \"features.5.1.mlp.3.bias\", \"features.5.2.norm1.weight\", \"features.5.2.norm1.bias\", \"features.5.2.attn.relative_position_bias_table\", \"features.5.2.attn.relative_position_index\", \"features.5.2.attn.qkv.weight\", \"features.5.2.attn.qkv.bias\", \"features.5.2.attn.proj.weight\", \"features.5.2.attn.proj.bias\", \"features.5.2.norm2.weight\", \"features.5.2.norm2.bias\", \"features.5.2.mlp.0.weight\", \"features.5.2.mlp.0.bias\", \"features.5.2.mlp.3.weight\", \"features.5.2.mlp.3.bias\", \"features.5.3.norm1.weight\", \"features.5.3.norm1.bias\", \"features.5.3.attn.relative_position_bias_table\", \"features.5.3.attn.relative_position_index\", \"features.5.3.attn.qkv.weight\", \"features.5.3.attn.qkv.bias\", \"features.5.3.attn.proj.weight\", \"features.5.3.attn.proj.bias\", \"features.5.3.norm2.weight\", \"features.5.3.norm2.bias\", \"features.5.3.mlp.0.weight\", \"features.5.3.mlp.0.bias\", \"features.5.3.mlp.3.weight\", \"features.5.3.mlp.3.bias\", \"features.5.4.norm1.weight\", \"features.5.4.norm1.bias\", \"features.5.4.attn.relative_position_bias_table\", \"features.5.4.attn.relative_position_index\", \"features.5.4.attn.qkv.weight\", \"features.5.4.attn.qkv.bias\", \"features.5.4.attn.proj.weight\", \"features.5.4.attn.proj.bias\", \"features.5.4.norm2.weight\", \"features.5.4.norm2.bias\", \"features.5.4.mlp.0.weight\", \"features.5.4.mlp.0.bias\", \"features.5.4.mlp.3.weight\", \"features.5.4.mlp.3.bias\", \"features.5.5.norm1.weight\", \"features.5.5.norm1.bias\", \"features.5.5.attn.relative_position_bias_table\", \"features.5.5.attn.relative_position_index\", \"features.5.5.attn.qkv.weight\", \"features.5.5.attn.qkv.bias\", \"features.5.5.attn.proj.weight\", \"features.5.5.attn.proj.bias\", \"features.5.5.norm2.weight\", \"features.5.5.norm2.bias\", \"features.5.5.mlp.0.weight\", \"features.5.5.mlp.0.bias\", \"features.5.5.mlp.3.weight\", \"features.5.5.mlp.3.bias\", \"features.6.reduction.weight\", \"features.6.norm.weight\", \"features.6.norm.bias\", \"features.7.0.norm1.weight\", \"features.7.0.norm1.bias\", \"features.7.0.attn.relative_position_bias_table\", \"features.7.0.attn.relative_position_index\", \"features.7.0.attn.qkv.weight\", \"features.7.0.attn.qkv.bias\", \"features.7.0.attn.proj.weight\", \"features.7.0.attn.proj.bias\", \"features.7.0.norm2.weight\", \"features.7.0.norm2.bias\", \"features.7.0.mlp.0.weight\", \"features.7.0.mlp.0.bias\", \"features.7.0.mlp.3.weight\", \"features.7.0.mlp.3.bias\", \"features.7.1.norm1.weight\", \"features.7.1.norm1.bias\", \"features.7.1.attn.relative_position_bias_table\", \"features.7.1.attn.relative_position_index\", \"features.7.1.attn.qkv.weight\", \"features.7.1.attn.qkv.bias\", \"features.7.1.attn.proj.weight\", \"features.7.1.attn.proj.bias\", \"features.7.1.norm2.weight\", \"features.7.1.norm2.bias\", \"features.7.1.mlp.0.weight\", \"features.7.1.mlp.0.bias\", \"features.7.1.mlp.3.weight\", \"features.7.1.mlp.3.bias\", \"norm.weight\", \"norm.bias\", \"head.weight\", \"head.bias\". \n\tUnexpected key(s) in state_dict: \"class_token\", \"conv_proj.weight\", \"conv_proj.bias\", \"encoder.pos_embedding\", \"encoder.layers.encoder_layer_0.ln_1.weight\", \"encoder.layers.encoder_layer_0.ln_1.bias\", \"encoder.layers.encoder_layer_0.self_attention.in_proj_weight\", \"encoder.layers.encoder_layer_0.self_attention.in_proj_bias\", \"encoder.layers.encoder_layer_0.self_attention.out_proj.weight\", \"encoder.layers.encoder_layer_0.self_attention.out_proj.bias\", \"encoder.layers.encoder_layer_0.ln_2.weight\", \"encoder.layers.encoder_layer_0.ln_2.bias\", \"encoder.layers.encoder_layer_0.mlp.0.weight\", \"encoder.layers.encoder_layer_0.mlp.0.bias\", \"encoder.layers.encoder_layer_0.mlp.3.weight\", \"encoder.layers.encoder_layer_0.mlp.3.bias\", \"encoder.layers.encoder_layer_1.ln_1.weight\", \"encoder.layers.encoder_layer_1.ln_1.bias\", \"encoder.layers.encoder_layer_1.self_attention.in_proj_weight\", \"encoder.layers.encoder_layer_1.self_attention.in_proj_bias\", \"encoder.layers.encoder_layer_1.self_attention.out_proj.weight\", \"encoder.layers.encoder_layer_1.self_attention.out_proj.bias\", \"encoder.layers.encoder_layer_1.ln_2.weight\", \"encoder.layers.encoder_layer_1.ln_2.bias\", \"encoder.layers.encoder_layer_1.mlp.0.weight\", \"encoder.layers.encoder_layer_1.mlp.0.bias\", \"encoder.layers.encoder_layer_1.mlp.3.weight\", \"encoder.layers.encoder_layer_1.mlp.3.bias\", \"encoder.layers.encoder_layer_2.ln_1.weight\", \"encoder.layers.encoder_layer_2.ln_1.bias\", \"encoder.layers.encoder_layer_2.self_attention.in_proj_weight\", \"encoder.layers.encoder_layer_2.self_attention.in_proj_bias\", \"encoder.layers.encoder_layer_2.self_attention.out_proj.weight\", \"encoder.layers.encoder_layer_2.self_attention.out_proj.bias\", \"encoder.layers.encoder_layer_2.ln_2.weight\", \"encoder.layers.encoder_layer_2.ln_2.bias\", \"encoder.layers.encoder_layer_2.mlp.0.weight\", \"encoder.layers.encoder_layer_2.mlp.0.bias\", \"encoder.layers.encoder_layer_2.mlp.3.weight\", \"encoder.layers.encoder_layer_2.mlp.3.bias\", \"encoder.layers.encoder_layer_3.ln_1.weight\", \"encoder.layers.encoder_layer_3.ln_1.bias\", \"encoder.layers.encoder_layer_3.self_attention.in_proj_weight\", \"encoder.layers.encoder_layer_3.self_attention.in_proj_bias\", \"encoder.layers.encoder_layer_3.self_attention.out_proj.weight\", \"encoder.layers.encoder_layer_3.self_attention.out_proj.bias\", \"encoder.layers.encoder_layer_3.ln_2.weight\", \"encoder.layers.encoder_layer_3.ln_2.bias\", \"encoder.layers.encoder_layer_3.mlp.0.weight\", \"encoder.layers.encoder_layer_3.mlp.0.bias\", \"encoder.layers.encoder_layer_3.mlp.3.weight\", \"encoder.layers.encoder_layer_3.mlp.3.bias\", \"encoder.layers.encoder_layer_4.ln_1.weight\", \"encoder.layers.encoder_layer_4.ln_1.bias\", \"encoder.layers.encoder_layer_4.self_attention.in_proj_weight\", \"encoder.layers.encoder_layer_4.self_attention.in_proj_bias\", \"encoder.layers.encoder_layer_4.self_attention.out_proj.weight\", \"encoder.layers.encoder_layer_4.self_attention.out_proj.bias\", \"encoder.layers.encoder_layer_4.ln_2.weight\", \"encoder.layers.encoder_layer_4.ln_2.bias\", \"encoder.layers.encoder_layer_4.mlp.0.weight\", \"encoder.layers.encoder_layer_4.mlp.0.bias\", \"encoder.layers.encoder_layer_4.mlp.3.weight\", \"encoder.layers.encoder_layer_4.mlp.3.bias\", \"encoder.layers.encoder_layer_5.ln_1.weight\", \"encoder.layers.encoder_layer_5.ln_1.bias\", \"encoder.layers.encoder_layer_5.self_attention.in_proj_weight\", \"encoder.layers.encoder_layer_5.self_attention.in_proj_bias\", \"encoder.layers.encoder_layer_5.self_attention.out_proj.weight\", \"encoder.layers.encoder_layer_5.self_attention.out_proj.bias\", \"encoder.layers.encoder_layer_5.ln_2.weight\", \"encoder.layers.encoder_layer_5.ln_2.bias\", \"encoder.layers.encoder_layer_5.mlp.0.weight\", \"encoder.layers.encoder_layer_5.mlp.0.bias\", \"encoder.layers.encoder_layer_5.mlp.3.weight\", \"encoder.layers.encoder_layer_5.mlp.3.bias\", \"encoder.layers.encoder_layer_6.ln_1.weight\", \"encoder.layers.encoder_layer_6.ln_1.bias\", \"encoder.layers.encoder_layer_6.self_attention.in_proj_weight\", \"encoder.layers.encoder_layer_6.self_attention.in_proj_bias\", \"encoder.layers.encoder_layer_6.self_attention.out_proj.weight\", \"encoder.layers.encoder_layer_6.self_attention.out_proj.bias\", \"encoder.layers.encoder_layer_6.ln_2.weight\", \"encoder.layers.encoder_layer_6.ln_2.bias\", \"encoder.layers.encoder_layer_6.mlp.0.weight\", \"encoder.layers.encoder_layer_6.mlp.0.bias\", \"encoder.layers.encoder_layer_6.mlp.3.weight\", \"encoder.layers.encoder_layer_6.mlp.3.bias\", \"encoder.layers.encoder_layer_7.ln_1.weight\", \"encoder.layers.encoder_layer_7.ln_1.bias\", \"encoder.layers.encoder_layer_7.self_attention.in_proj_weight\", \"encoder.layers.encoder_layer_7.self_attention.in_proj_bias\", \"encoder.layers.encoder_layer_7.self_attention.out_proj.weight\", \"encoder.layers.encoder_layer_7.self_attention.out_proj.bias\", \"encoder.layers.encoder_layer_7.ln_2.weight\", \"encoder.layers.encoder_layer_7.ln_2.bias\", \"encoder.layers.encoder_layer_7.mlp.0.weight\", \"encoder.layers.encoder_layer_7.mlp.0.bias\", \"encoder.layers.encoder_layer_7.mlp.3.weight\", \"encoder.layers.encoder_layer_7.mlp.3.bias\", \"encoder.layers.encoder_layer_8.ln_1.weight\", \"encoder.layers.encoder_layer_8.ln_1.bias\", \"encoder.layers.encoder_layer_8.self_attention.in_proj_weight\", \"encoder.layers.encoder_layer_8.self_attention.in_proj_bias\", \"encoder.layers.encoder_layer_8.self_attention.out_proj.weight\", \"encoder.layers.encoder_layer_8.self_attention.out_proj.bias\", \"encoder.layers.encoder_layer_8.ln_2.weight\", \"encoder.layers.encoder_layer_8.ln_2.bias\", \"encoder.layers.encoder_layer_8.mlp.0.weight\", \"encoder.layers.encoder_layer_8.mlp.0.bias\", \"encoder.layers.encoder_layer_8.mlp.3.weight\", \"encoder.layers.encoder_layer_8.mlp.3.bias\", \"encoder.layers.encoder_layer_9.ln_1.weight\", \"encoder.layers.encoder_layer_9.ln_1.bias\", \"encoder.layers.encoder_layer_9.self_attention.in_proj_weight\", \"encoder.layers.encoder_layer_9.self_attention.in_proj_bias\", \"encoder.layers.encoder_layer_9.self_attention.out_proj.weight\", \"encoder.layers.encoder_layer_9.self_attention.out_proj.bias\", \"encoder.layers.encoder_layer_9.ln_2.weight\", \"encoder.layers.encoder_layer_9.ln_2.bias\", \"encoder.layers.encoder_layer_9.mlp.0.weight\", \"encoder.layers.encoder_layer_9.mlp.0.bias\", \"encoder.layers.encoder_layer_9.mlp.3.weight\", \"encoder.layers.encoder_layer_9.mlp.3.bias\", \"encoder.layers.encoder_layer_10.ln_1.weight\", \"encoder.layers.encoder_layer_10.ln_1.bias\", \"encoder.layers.encoder_layer_10.self_attention.in_proj_weight\", \"encoder.layers.encoder_layer_10.self_attention.in_proj_bias\", \"encoder.layers.encoder_layer_10.self_attention.out_proj.weight\", \"encoder.layers.encoder_layer_10.self_attention.out_proj.bias\", \"encoder.layers.encoder_layer_10.ln_2.weight\", \"encoder.layers.encoder_layer_10.ln_2.bias\", \"encoder.layers.encoder_layer_10.mlp.0.weight\", \"encoder.layers.encoder_layer_10.mlp.0.bias\", \"encoder.layers.encoder_layer_10.mlp.3.weight\", \"encoder.layers.encoder_layer_10.mlp.3.bias\", \"encoder.layers.encoder_layer_11.ln_1.weight\", \"encoder.layers.encoder_layer_11.ln_1.bias\", \"encoder.layers.encoder_layer_11.self_attention.in_proj_weight\", \"encoder.layers.encoder_layer_11.self_attention.in_proj_bias\", \"encoder.layers.encoder_layer_11.self_attention.out_proj.weight\", \"encoder.layers.encoder_layer_11.self_attention.out_proj.bias\", \"encoder.layers.encoder_layer_11.ln_2.weight\", \"encoder.layers.encoder_layer_11.ln_2.bias\", \"encoder.layers.encoder_layer_11.mlp.0.weight\", \"encoder.layers.encoder_layer_11.mlp.0.bias\", \"encoder.layers.encoder_layer_11.mlp.3.weight\", \"encoder.layers.encoder_layer_11.mlp.3.bias\", \"encoder.ln.weight\", \"encoder.ln.bias\", \"heads.head.weight\", \"heads.head.bias\". "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision.models import vit_b_16\n",
    "from PIL import Image\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torchvision.models import swin_t, Swin_T_Weights\n",
    "\n",
    "# Set up the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Function to create the model (same as in the training script)\n",
    "def create_model(num_classes):\n",
    "    model = vit_b_16()\n",
    "    \n",
    "    # Replace the last layer of the classifier\n",
    "    num_ftrs = model.heads[-1].in_features\n",
    "    model.heads[-1] = nn.Linear(num_ftrs, num_classes)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "num_classes = 42  # Make sure this matches the number of classes in your trained model\n",
    "model = create_model(num_classes)\n",
    "\n",
    "# Load the state dictionary\n",
    "state_dict = torch.load('best_model_vit_h14.pth', map_location=device)\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Set up the image transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Function to predict the class of a single image\n",
    "def predict_image(image_path, model, transform):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "    return predicted.item()\n",
    "\n",
    "# Read the test.csv file\n",
    "test_csv_path = 'test.csv'\n",
    "df = pd.read_csv(test_csv_path)\n",
    "\n",
    "# Ensure the first column contains image names\n",
    "image_column = df.columns[0]\n",
    "image_names = df[image_column].tolist()\n",
    "\n",
    "# Set the path to the test folder containing the images\n",
    "test_folder = 'Images'  # Replace with your test folder path if different\n",
    "\n",
    "# Predict classes for all images and store results\n",
    "results = []\n",
    "for image_name in tqdm(image_names, desc=\"Predicting\"):\n",
    "    # Add .jpg extension to the image name\n",
    "    image_file = image_name + '.jpg'\n",
    "    image_path = os.path.join(test_folder, image_file)\n",
    "    \n",
    "    # Check if the image file exists\n",
    "    if os.path.exists(image_path):\n",
    "        predicted_class = predict_image(image_path, model, transform)\n",
    "        results.append((image_name, predicted_class))\n",
    "    else:\n",
    "        print(f\"Warning: Image {image_file} not found in {test_folder}\")\n",
    "        results.append((image_name, \"Not Found\"))\n",
    "\n",
    "# Write results to CSV file\n",
    "output_csv = 'predictions2.csv'\n",
    "with open(output_csv, 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow(['Image Name', 'Predicted Class'])  # Write header\n",
    "    csv_writer.writerows(results)\n",
    "\n",
    "print(f\"Predictions completed. Results saved to {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af64ea3-a06d-4f34-a00b-4df94489826d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
